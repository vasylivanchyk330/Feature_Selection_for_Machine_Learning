{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest importance\n",
    "\n",
    "Here, it is demonstrated how to select features based on tree importance using a regression and classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 109)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('../dataset_2.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>...</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.532710</td>\n",
       "      <td>3.280834</td>\n",
       "      <td>17.982476</td>\n",
       "      <td>4.404259</td>\n",
       "      <td>2.349910</td>\n",
       "      <td>0.603264</td>\n",
       "      <td>2.784655</td>\n",
       "      <td>0.323146</td>\n",
       "      <td>12.009691</td>\n",
       "      <td>0.139346</td>\n",
       "      <td>...</td>\n",
       "      <td>2.079066</td>\n",
       "      <td>6.748819</td>\n",
       "      <td>2.941445</td>\n",
       "      <td>18.360496</td>\n",
       "      <td>17.726613</td>\n",
       "      <td>7.774031</td>\n",
       "      <td>1.473441</td>\n",
       "      <td>1.973832</td>\n",
       "      <td>0.976806</td>\n",
       "      <td>2.541417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.821374</td>\n",
       "      <td>12.098722</td>\n",
       "      <td>13.309151</td>\n",
       "      <td>4.125599</td>\n",
       "      <td>1.045386</td>\n",
       "      <td>1.832035</td>\n",
       "      <td>1.833494</td>\n",
       "      <td>0.709090</td>\n",
       "      <td>8.652883</td>\n",
       "      <td>0.102757</td>\n",
       "      <td>...</td>\n",
       "      <td>2.479789</td>\n",
       "      <td>7.795290</td>\n",
       "      <td>3.557890</td>\n",
       "      <td>17.383378</td>\n",
       "      <td>15.193423</td>\n",
       "      <td>8.263673</td>\n",
       "      <td>1.878108</td>\n",
       "      <td>0.567939</td>\n",
       "      <td>1.018818</td>\n",
       "      <td>1.416433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.938776</td>\n",
       "      <td>7.952752</td>\n",
       "      <td>0.972671</td>\n",
       "      <td>3.459267</td>\n",
       "      <td>1.935782</td>\n",
       "      <td>0.621463</td>\n",
       "      <td>2.338139</td>\n",
       "      <td>0.344948</td>\n",
       "      <td>9.937850</td>\n",
       "      <td>11.691283</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861487</td>\n",
       "      <td>6.130886</td>\n",
       "      <td>3.401064</td>\n",
       "      <td>15.850471</td>\n",
       "      <td>14.620599</td>\n",
       "      <td>6.849776</td>\n",
       "      <td>1.098210</td>\n",
       "      <td>1.959183</td>\n",
       "      <td>1.575493</td>\n",
       "      <td>1.857893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.020690</td>\n",
       "      <td>9.900544</td>\n",
       "      <td>17.869637</td>\n",
       "      <td>4.366715</td>\n",
       "      <td>1.973693</td>\n",
       "      <td>2.026012</td>\n",
       "      <td>2.853025</td>\n",
       "      <td>0.674847</td>\n",
       "      <td>11.816859</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>...</td>\n",
       "      <td>1.340944</td>\n",
       "      <td>7.240058</td>\n",
       "      <td>2.417235</td>\n",
       "      <td>15.194609</td>\n",
       "      <td>13.553772</td>\n",
       "      <td>7.229971</td>\n",
       "      <td>0.835158</td>\n",
       "      <td>2.234482</td>\n",
       "      <td>0.946170</td>\n",
       "      <td>2.700606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.909506</td>\n",
       "      <td>10.576516</td>\n",
       "      <td>0.934191</td>\n",
       "      <td>3.419572</td>\n",
       "      <td>1.871438</td>\n",
       "      <td>3.340811</td>\n",
       "      <td>1.868282</td>\n",
       "      <td>0.439865</td>\n",
       "      <td>13.585620</td>\n",
       "      <td>1.153366</td>\n",
       "      <td>...</td>\n",
       "      <td>2.738095</td>\n",
       "      <td>6.565509</td>\n",
       "      <td>4.341414</td>\n",
       "      <td>15.893832</td>\n",
       "      <td>11.929787</td>\n",
       "      <td>6.954033</td>\n",
       "      <td>1.853364</td>\n",
       "      <td>0.511027</td>\n",
       "      <td>2.599562</td>\n",
       "      <td>0.811364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var_1      var_2      var_3     var_4     var_5     var_6     var_7  \\\n",
       "0  4.532710   3.280834  17.982476  4.404259  2.349910  0.603264  2.784655   \n",
       "1  5.821374  12.098722  13.309151  4.125599  1.045386  1.832035  1.833494   \n",
       "2  1.938776   7.952752   0.972671  3.459267  1.935782  0.621463  2.338139   \n",
       "3  6.020690   9.900544  17.869637  4.366715  1.973693  2.026012  2.853025   \n",
       "4  3.909506  10.576516   0.934191  3.419572  1.871438  3.340811  1.868282   \n",
       "\n",
       "      var_8      var_9     var_10  ...   var_100   var_101   var_102  \\\n",
       "0  0.323146  12.009691   0.139346  ...  2.079066  6.748819  2.941445   \n",
       "1  0.709090   8.652883   0.102757  ...  2.479789  7.795290  3.557890   \n",
       "2  0.344948   9.937850  11.691283  ...  1.861487  6.130886  3.401064   \n",
       "3  0.674847  11.816859   0.011151  ...  1.340944  7.240058  2.417235   \n",
       "4  0.439865  13.585620   1.153366  ...  2.738095  6.565509  4.341414   \n",
       "\n",
       "     var_103    var_104   var_105   var_106   var_107   var_108   var_109  \n",
       "0  18.360496  17.726613  7.774031  1.473441  1.973832  0.976806  2.541417  \n",
       "1  17.383378  15.193423  8.263673  1.878108  0.567939  1.018818  1.416433  \n",
       "2  15.850471  14.620599  6.849776  1.098210  1.959183  1.575493  1.857893  \n",
       "3  15.194609  13.553772  7.229971  0.835158  2.234482  0.946170  2.700606  \n",
       "4  15.893832  11.929787  6.954033  1.853364  0.511027  2.599562  0.811364  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 108), (15000, 108))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features with tree importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(n_estimators=10,\n",
       "                                                 random_state=10))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_ = SelectFromModel(RandomForestClassifier(n_estimators=10, random_state=10))\n",
    "\n",
    "sel_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var_1', 'var_2', 'var_6', 'var_9', 'var_13', 'var_15', 'var_16',\n",
       "       'var_17', 'var_20', 'var_21', 'var_30', 'var_34', 'var_37', 'var_55',\n",
       "       'var_60', 'var_67', 'var_69', 'var_70', 'var_71', 'var_82', 'var_87',\n",
       "       'var_88', 'var_95', 'var_96', 'var_99', 'var_103', 'var_108'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# and now let's plot the distribution of importances\n",
    "\n",
    "pd.Series(sel_.estimator_.feature_importances_.ravel()).hist(bins=20)\n",
    "plt.xlabel('Feature importance')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 108\n",
      "selected features: 27\n",
      "features with importance greater than the mean importance of all features: 27\n"
     ]
    }
   ],
   "source": [
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "\n",
    "print(\n",
    "    'features with importance greater than the mean importance of all features: {}'.format(\n",
    "        np.sum(sel_.estimator_.feature_importances_ >\n",
    "               sel_.estimator_.feature_importances_.mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note:\n",
    "\n",
    "- If we change the parameters of the tree, we may obtain different features\n",
    "- How many features to select is somewhat arbitrary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('../houseprice.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 38)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1022, 37), (438, 37))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['SalePrice'], axis=1),\n",
    "    data['SalePrice'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features with tree importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestRegressor(random_state=10))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_ = SelectFromModel(RandomForestRegressor(n_estimators=100, random_state=10))\n",
    "sel_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a list and count the selected features\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 37\n",
      "selected features: 5\n",
      "features with coefficients greater than the mean coefficient: 5\n"
     ]
    }
   ],
   "source": [
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "\n",
    "print(\n",
    "    'features with coefficients greater than the mean coefficient: {}'.format(\n",
    "        np.sum(sel_.estimator_.feature_importances_ >\n",
    "               sel_.estimator_.feature_importances_.mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting features by using tree derived feature importance is a very straightforward, fast and generally accurate way of selecting good features for machine learning. However, correlated features will show in a tree similar importance, but lower than compared to what their importance would be if the tree was built without the correlated counterparts. In situations like this, it might be better to select features recursively, rather than altogether it has been just done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recursive Feature Selection using Random Forests importance\n",
    "\n",
    "Random Forests assign equal or similar importance to features that are highly correlated. In addition, when features are correlated, the importance assigned is lower than the importance attributed to the feature itself, should the tree be built without the correlated counterparts.\n",
    "\n",
    "Therefore, instead of eliminating features based on importance by brute force it is better to remove one feature at a time, and recalculating the importance on each round. This procedure is called Recursive Feature Elimination (RFE)\n",
    "\n",
    "RFE is a hybrid between embedded and wrapper methods: it is based on computation derived when fitting the model, but it also requires fitting several models.\n",
    "\n",
    "The cycle is as follows:\n",
    "\n",
    "- Build Random Forests using all features\n",
    "- Remove least important feature\n",
    "- Build Random Forests and recalculate importance\n",
    "- Repeat until a criteria is met\n",
    "\n",
    "In this situation, when a feature that is highly correlated to another one is removed, then, the importance of the remaining feature increases. This may lead to a better feature space selection. On the downside, building several Random Forests is quite time and compute resource consuming, in particular if the dataset contains a high number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 109)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('../dataset_2.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>...</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.532710</td>\n",
       "      <td>3.280834</td>\n",
       "      <td>17.982476</td>\n",
       "      <td>4.404259</td>\n",
       "      <td>2.349910</td>\n",
       "      <td>0.603264</td>\n",
       "      <td>2.784655</td>\n",
       "      <td>0.323146</td>\n",
       "      <td>12.009691</td>\n",
       "      <td>0.139346</td>\n",
       "      <td>...</td>\n",
       "      <td>2.079066</td>\n",
       "      <td>6.748819</td>\n",
       "      <td>2.941445</td>\n",
       "      <td>18.360496</td>\n",
       "      <td>17.726613</td>\n",
       "      <td>7.774031</td>\n",
       "      <td>1.473441</td>\n",
       "      <td>1.973832</td>\n",
       "      <td>0.976806</td>\n",
       "      <td>2.541417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.821374</td>\n",
       "      <td>12.098722</td>\n",
       "      <td>13.309151</td>\n",
       "      <td>4.125599</td>\n",
       "      <td>1.045386</td>\n",
       "      <td>1.832035</td>\n",
       "      <td>1.833494</td>\n",
       "      <td>0.709090</td>\n",
       "      <td>8.652883</td>\n",
       "      <td>0.102757</td>\n",
       "      <td>...</td>\n",
       "      <td>2.479789</td>\n",
       "      <td>7.795290</td>\n",
       "      <td>3.557890</td>\n",
       "      <td>17.383378</td>\n",
       "      <td>15.193423</td>\n",
       "      <td>8.263673</td>\n",
       "      <td>1.878108</td>\n",
       "      <td>0.567939</td>\n",
       "      <td>1.018818</td>\n",
       "      <td>1.416433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.938776</td>\n",
       "      <td>7.952752</td>\n",
       "      <td>0.972671</td>\n",
       "      <td>3.459267</td>\n",
       "      <td>1.935782</td>\n",
       "      <td>0.621463</td>\n",
       "      <td>2.338139</td>\n",
       "      <td>0.344948</td>\n",
       "      <td>9.937850</td>\n",
       "      <td>11.691283</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861487</td>\n",
       "      <td>6.130886</td>\n",
       "      <td>3.401064</td>\n",
       "      <td>15.850471</td>\n",
       "      <td>14.620599</td>\n",
       "      <td>6.849776</td>\n",
       "      <td>1.098210</td>\n",
       "      <td>1.959183</td>\n",
       "      <td>1.575493</td>\n",
       "      <td>1.857893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.020690</td>\n",
       "      <td>9.900544</td>\n",
       "      <td>17.869637</td>\n",
       "      <td>4.366715</td>\n",
       "      <td>1.973693</td>\n",
       "      <td>2.026012</td>\n",
       "      <td>2.853025</td>\n",
       "      <td>0.674847</td>\n",
       "      <td>11.816859</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>...</td>\n",
       "      <td>1.340944</td>\n",
       "      <td>7.240058</td>\n",
       "      <td>2.417235</td>\n",
       "      <td>15.194609</td>\n",
       "      <td>13.553772</td>\n",
       "      <td>7.229971</td>\n",
       "      <td>0.835158</td>\n",
       "      <td>2.234482</td>\n",
       "      <td>0.946170</td>\n",
       "      <td>2.700606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.909506</td>\n",
       "      <td>10.576516</td>\n",
       "      <td>0.934191</td>\n",
       "      <td>3.419572</td>\n",
       "      <td>1.871438</td>\n",
       "      <td>3.340811</td>\n",
       "      <td>1.868282</td>\n",
       "      <td>0.439865</td>\n",
       "      <td>13.585620</td>\n",
       "      <td>1.153366</td>\n",
       "      <td>...</td>\n",
       "      <td>2.738095</td>\n",
       "      <td>6.565509</td>\n",
       "      <td>4.341414</td>\n",
       "      <td>15.893832</td>\n",
       "      <td>11.929787</td>\n",
       "      <td>6.954033</td>\n",
       "      <td>1.853364</td>\n",
       "      <td>0.511027</td>\n",
       "      <td>2.599562</td>\n",
       "      <td>0.811364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var_1      var_2      var_3     var_4     var_5     var_6     var_7  \\\n",
       "0  4.532710   3.280834  17.982476  4.404259  2.349910  0.603264  2.784655   \n",
       "1  5.821374  12.098722  13.309151  4.125599  1.045386  1.832035  1.833494   \n",
       "2  1.938776   7.952752   0.972671  3.459267  1.935782  0.621463  2.338139   \n",
       "3  6.020690   9.900544  17.869637  4.366715  1.973693  2.026012  2.853025   \n",
       "4  3.909506  10.576516   0.934191  3.419572  1.871438  3.340811  1.868282   \n",
       "\n",
       "      var_8      var_9     var_10  ...   var_100   var_101   var_102  \\\n",
       "0  0.323146  12.009691   0.139346  ...  2.079066  6.748819  2.941445   \n",
       "1  0.709090   8.652883   0.102757  ...  2.479789  7.795290  3.557890   \n",
       "2  0.344948   9.937850  11.691283  ...  1.861487  6.130886  3.401064   \n",
       "3  0.674847  11.816859   0.011151  ...  1.340944  7.240058  2.417235   \n",
       "4  0.439865  13.585620   1.153366  ...  2.738095  6.565509  4.341414   \n",
       "\n",
       "     var_103    var_104   var_105   var_106   var_107   var_108   var_109  \n",
       "0  18.360496  17.726613  7.774031  1.473441  1.973832  0.976806  2.541417  \n",
       "1  17.383378  15.193423  8.263673  1.878108  0.567939  1.018818  1.416433  \n",
       "2  15.850471  14.620599  6.849776  1.098210  1.959183  1.575493  1.857893  \n",
       "3  15.194609  13.553772  7.229971  0.835158  2.234482  0.946170  2.700606  \n",
       "4  15.893832  11.929787  6.954033  1.853364  0.511027  2.599562  0.811364  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 108), (15000, 108))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFE(estimator=RandomForestClassifier(n_estimators=10, random_state=10),\n",
       "    n_features_to_select=27)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_ = RFE(RandomForestClassifier(n_estimators=10, random_state=10), n_features_to_select=27)\n",
    "sel_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selected features\n",
    "\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var_4', 'var_14', 'var_21', 'var_25', 'var_30', 'var_31', 'var_34',\n",
       "       'var_38', 'var_41', 'var_46', 'var_53', 'var_55', 'var_56', 'var_70',\n",
       "       'var_71', 'var_73', 'var_79', 'var_82', 'var_86', 'var_87', 'var_88',\n",
       "       'var_90', 'var_92', 'var_93', 'var_96', 'var_104', 'var_108'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's display the list of features\n",
    "selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select previously selected features for further comparing\n",
    "\n",
    "previous_lecture_selected_features = [\n",
    "    'var_1', 'var_2', 'var_6', 'var_9', 'var_13', 'var_15', 'var_16', 'var_17',\n",
    "    'var_20', 'var_21', 'var_30', 'var_34', 'var_37', 'var_55', 'var_60',\n",
    "    'var_67', 'var_69', 'var_70', 'var_71', 'var_82', 'var_87', 'var_88',\n",
    "    'var_95', 'var_96', 'var_99', 'var_103', 'var_108'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare performance of feature subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to build random forests and compare\n",
    "# their performance in train and test sets\n",
    "\n",
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    print('Train set')\n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    \n",
    "    print('Test set')\n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.7046591498448564\n",
      "Test set\n",
      "Random Forests roc-auc: 0.6904472637540937\n"
     ]
    }
   ],
   "source": [
    "# features selected recursively\n",
    "run_randomForests(X_train[selected_feat],\n",
    "                  X_test[selected_feat],\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.7126509093226299\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7009195513033531\n"
     ]
    }
   ],
   "source": [
    "# features selected altogether\n",
    "run_randomForests(X_train[previous_lecture_selected_features],\n",
    "                  X_test[previous_lecture_selected_features],\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that RFE did not return a better subset of features. The performance of the model built using the variables selected directly from the first RandomForest was enough to find a good subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature selection with decision trees in pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 301)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../dataset_1.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 300), (15000, 300))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy for further comperison\n",
    "\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 266), (15000, 266))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_features = [\n",
    "    feat for feat in X_train.columns if X_train[feat].std() == 0\n",
    "]\n",
    "\n",
    "X_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=constant_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove quasi-constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = VarianceThreshold(threshold=0.01)  \n",
    "\n",
    "sel.fit(X_train)\n",
    "sum(sel.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = X_train.columns[sel.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 215), (15000, 215))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the features\n",
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= pd.DataFrame(X_train)\n",
    "X_train.columns = features_to_keep\n",
    "\n",
    "X_test= pd.DataFrame(X_test)\n",
    "X_test.columns = features_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicated features in the training set\n",
    "duplicated_feat = []\n",
    "for i in range(0, len(X_train.columns)):\n",
    "\n",
    "    col_1 = X_train.columns[i]\n",
    "\n",
    "    for col_2 in X_train.columns[i + 1:]:\n",
    "        if X_train[col_1].equals(X_train[col_2]):\n",
    "            duplicated_feat.append(col_2)\n",
    "            \n",
    "len(duplicated_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 205), (15000, 205))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicated features\n",
    "\n",
    "X_train.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "X_test.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy for further comperison\n",
    "\n",
    "X_train_basic_filter = X_train.copy()\n",
    "X_test_basic_filter = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  93\n"
     ]
    }
   ],
   "source": [
    "# find and remove correlated features\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colname = corr_matrix.columns[i]\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 112), (15000, 112))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removed correlated  features\n",
    "\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy\n",
    "\n",
    "X_train_corr = X_train.copy()\n",
    "X_test_corr = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features by Random Forests Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_ = SelectFromModel(RandomForestClassifier(n_estimators=50, random_state=10))\n",
    "sel_.fit(X_train, y_train)\n",
    "\n",
    "# remove features with zero coefficient\n",
    "X_train_rf = pd.DataFrame(sel_.transform(X_train))\n",
    "X_test_rf = pd.DataFrame(sel_.transform(X_test))\n",
    "\n",
    "# add the columns name\n",
    "X_train_rf.columns = X_train.columns[(sel_.get_support())]\n",
    "X_test_rf.columns = X_train.columns[(sel_.get_support())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 16), (15000, 16))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rf.shape, X_test_rf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the performance in machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to build random forests and compare performance in train and test set\n",
    "\n",
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    print('Train set')\n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    \n",
    "    print('Test set')\n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.807612232524249\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7868832427636059\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "run_randomForests(X_train_original,\n",
    "                  X_test_original,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.810290026780428\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7914020645941601\n"
     ]
    }
   ],
   "source": [
    "# filter methods - basic\n",
    "run_randomForests(X_train_basic_filter,\n",
    "                  X_test_basic_filter,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.8066004772684517\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7859521124929707\n"
     ]
    }
   ],
   "source": [
    "# filter methods - correlation\n",
    "run_randomForests(X_train_corr,\n",
    "                  X_test_corr,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.8066004772684517\n",
      "Test set\n",
      "Random Forests roc-auc: 0.7859521124929707\n"
     ]
    }
   ],
   "source": [
    "# filter methods - univariate roc-auc\n",
    "run_randomForests(X_train_corr,\n",
    "                  X_test_corr,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.825594244784318\n",
      "Test set\n",
      "Random Forests roc-auc: 0.8037861254524954\n"
     ]
    }
   ],
   "source": [
    "# embedded methods - Random forests\n",
    "run_randomForests(X_train_rf,\n",
    "                  X_test_rf,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests built using 16 features display a slightly higher performance than a model built with all features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
